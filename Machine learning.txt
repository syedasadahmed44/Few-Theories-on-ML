1. What are the three stages to build the hypotheses or model in machine learning?

Answer-1)
Three stages to build the hypotheses or model :
a) Model building
b) Model testing
c) Applying the model
Brief information on these three stages are as below :
a) Model building:
This step involves choosing the appropriate algorithm and representation of data in the form of the model. The model building process involves setting up 
ways of collecting data, understanding and paying attention to what is important in the data to answer the questions we are asking, finding a statistical,
mathematical or a simulation model to gain understanding and make predictions.
Steps to be followed for "Data Preparation Process" which is used for model building : Step 1: Select Data
=> selecting the subset of all available data that we will be working with. Step 2: Preprocess Data => Perform data preprocessing steps such as formatting,
 cleaning and sampling to get the format of the data which we want to work with. Step 3: Transform Data
=> This step is also referred to as feature engineering. Three common data transformations techniques are scaling, attribute decompositions and attribute
 aggregations.
b) Model testing :
To test our model, we need to split our dataset into training and test dataset. A training set is a dataset used to train a model. To train our model, first 
we need to select features from the selected data and then apply the model so that it can learn from the training dataset. Then those learning will be applied
 to "Test dataset" to evalute our model.
c) Applying the model :
After the steps of "Model building" and "Model testing", we can apply this model to predict the outcome for the real time datasets.


################################################################################################################################################################

2. What is the standard approach to supervised learning?

Answer-2)
The standard approach to supervised learning is to split the set of example into the training set and the test set to evalute our model selection and then
 apply the model which is more accurate & efficient and overcome the issues such as underfitting & overfitting.


################################################################################################################################################################

3. What is Training set and Test set?

Answer-3)
Training Set :
It is part of dataset which are used to learn the behaviour of the data using the model selected.
Test Set :
Test set is used to test the accuracy of the hypotheses generated using the training set.

###############################################################################################################################################################


4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?

Answer-4)
General principle of an ensemble method :
Ensemble model combines multiple "individual" (diverse) models together and delivers superior prediction power.
Bagging :
It is an ensemble method in which first, we create random samples of the training data set (sub sets of training data set). Then, we build a classifier for each 
sample. Finally, results of these multiple classifiers are combined using average or majority voting. Bagging helps to reduce the variance error.
Boosting :
It provides sequential learning of the predictors. The first predictor is learned on the whole data set, while the following are learnt on the training set based
 on the performance of the previous one. It starts by classifying original data set and giving equal weights to each observation. If classes are predicted incorrectly
 using the first learner, then it gives higher weight to the missed classified observation. Being an iterative process, it continues to add classifier learner until a
 limit is reached in the number of models or accuracy. Boosting in general decreases the bias error and builds strong predictive models. However, they may sometimes
 over fit on the training data.


##############################################################################################################################################################

5. How can you avoid overfitting ?

Answer-5)
Steps for reducing overfitting:
1. Add more data
2. Use data augmentation
3. Use architectures that generalize well
4. Add regularization (mostly dropout, L1/L2 regularization are also possible)
5. Reduce architecture complexity.
Brief description on the above mentioned steps are as below :
The first step is of course to collect more data. However, in most cases we will not be able to. Let’s assume we have collected all the data. The next step is 
data augmentation: something that is always recommended to use. Data augmentation includes things like randomly rotating the image, zooming in, adding a color
 filter etc. Data augmentation only happens to the training set and not on the validation/test set. It can be useful to check if we are using too much data 
augmentation. For example, if we zoom in so much that features of a cat are not visible anymore, than the model is not going to get better from training on these
 images.
The third step is to use an architecture that generalizes well. However, much more important is the fourth step: adding regularization. The three most popular 
options are: dropout, L1 regularization and L2 regularization. Dropout deletes a random sample of the activations (makes them zero) in training. In the Vgg model
 this is only applied in the fully connected layers at the end of the model. However, it can also be applied to the convolutional layers. We need to be aware that
 dropout causes information to get lost.